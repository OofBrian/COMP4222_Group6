# rag/evaluate_rag.py
import json
import re
import numpy as np
from typing import List, Tuple, Dict, Any
from Ai import generate
from embedding import embed_chunk
from encoder import rerank
from evaluation import QAExactMatch, QAF1Score
from chromadb.errors import NotFoundError
import chromadb

# === Chroma Client ===
chromadb_client = chromadb.PersistentClient("./chroma.db")
TEMP_COLLECTION_NAME = "hotpotqa_temp_eval"


# === Safe Temp Collection ===
def get_temp_collection():
    try:
        chromadb_client.delete_collection(name=TEMP_COLLECTION_NAME)
    except NotFoundError:
        pass
    return chromadb_client.create_collection(name=TEMP_COLLECTION_NAME)


# === Sentence Splitter ===
def split_into_sentences(text: str) -> List[str]:
    """Split text into sentences using regex (robust to quotes, etc.)"""
    if not text.strip():
        return []
    # Split on .!? followed by space+capital letter
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])|(?<=[.!?])$', text.strip())
    return [s.strip() for s in sentences if s.strip()]


# === Index Context (Supports New Format) ===
def index_context(context: List[Dict], collection) -> None:
    """
    context: List[Dict] with 'idx', 'title', 'text'
    """
    chunks = []
    embeddings = []
    ids = []
    metadatas = []

    for title_idx, item in enumerate(context):
        title = item.get("title", f"doc_{title_idx}")
        text = item.get("text", "")
        sentences = split_into_sentences(text)

        for sent_idx, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
            chunk_id = f"{title_idx}_{sent_idx}"
            chunks.append(sentence)
            embeddings.append(embed_chunk(sentence))
            ids.append(chunk_id)
            metadatas.append({
                "title": title,
                "sent_idx": sent_idx,
                "title_idx": title_idx,
                "source_idx": item.get("idx")
            })

    if chunks:
        collection.add(
            documents=chunks,
            embeddings=embeddings,
            ids=ids,
            metadatas=metadatas
        )


# === Retrieve ===
def retrieve(query: str, collection, top_k: int = 10) -> List[str]:
    q_emb = embed_chunk(query)
    results = collection.query(
        query_embeddings=[q_emb],
        n_results=top_k,
        include=["documents"]
    )
    return results['documents'][0]


# === Supporting Facts Evaluation ===
def supporting_facts_precision_recall_f1(
    pred_chunks: List[str],
    gold_sfs: List[Tuple[str, int]],
    context: List[Dict]
) -> Dict[str, float]:
    title_to_sentences = {}
    for item in context:
        title = item.get("title", "")
        sentences = split_into_sentences(item.get("text", ""))
        title_to_sentences[title] = sentences

    pred_sfs = set()
    for chunk in pred_chunks:
        for title, sentences in title_to_sentences.items():
            if chunk in sentences:
                sent_idx = sentences.index(chunk)
                pred_sfs.add((title, sent_idx))
                break

    gold_sfs_set = set((title, sent_idx) for title, sent_idx in gold_sfs)

    if not gold_sfs_set:
        return {"sf_precision": 1.0, "sf_recall": 1.0, "sf_f1": 1.0}

    tp = len(pred_sfs & gold_sfs_set)
    precision = tp / len(pred_sfs) if pred_sfs else 0.0
    recall = tp / len(gold_sfs_set)
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

    return {"sf_precision": precision, "sf_recall": recall, "sf_f1": f1}


# === Main ===
def main():
    # === 1. Load Corpus ===
    CORPUS_PATH = "data/hotpotqa_corpus.json"
    with open(CORPUS_PATH, "r", encoding="utf-8") as f:
        corpus_list = json.load(f)
    corpus = {doc["idx"]: doc for doc in corpus_list}
    print(f"Loaded {len(corpus)} documents from corpus.")

    # === 2. Load Evaluation Questions ===
    EVAL_PATH = "data/hotpotqa_eval.json"
    with open(EVAL_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"Loaded {len(data)} evaluation questions.\n")
    print("="*70)

    # === 3. Metrics ===
    em_metric = QAExactMatch()
    f1_metric = QAF1Score()
    answer_preds = []
    answer_golds = []
    sf_metrics = {"precision": [], "recall": [], "f1": []}

    # === 4. Evaluation Loop ===
    for idx, item in enumerate(data):
        q = item["question"]
        answer = item["answer"]
        context_indices = item["context_indices"]
        supporting_facts = item.get("supporting_facts", [])

        # Build context from corpus
        context = []
        for idx_val in context_indices:
            if idx_val in corpus:
                context.append(corpus[idx_val])
            else:
                print(f"Warning: context index {idx_val} not in corpus")

        answer_golds.append([answer])

        # Fresh collection per question
        collection = get_temp_collection()
        index_context(context, collection)

        # Retrieve → Rerank → Generate
        retrieved_chunks = retrieve(q, collection, top_k=10)
        reranked_chunks = rerank(q, retrieved_chunks, top_k=3)
        generated_answer = generate(q, reranked_chunks)
        answer_preds.append(generated_answer)

        # Supporting Facts
        sf_scores = supporting_facts_precision_recall_f1(
            pred_chunks=reranked_chunks,
            gold_sfs=supporting_facts,
            context=context
        )
        sf_metrics["precision"].append(sf_scores["sf_precision"])
        sf_metrics["recall"].append(sf_scores["sf_recall"])
        sf_metrics["f1"].append(sf_scores["sf_f1"])

        # Print progress
        print(f"[{idx+1:4d}/{len(data)}] Q: {q}")
        print(f"     Gold: {answer}")
        print(f"     Pred: {generated_answer}")
        print(f"     SF F1: {sf_scores['sf_f1']:.3f} "
              f"(P:{sf_scores['sf_precision']:.3f}, R:{sf_scores['sf_recall']:.3f})\n")

    # === 5. Final Metrics ===
    em_res, _ = em_metric.calculate_metric_scores(answer_golds, answer_preds)
    f1_res, _ = f1_metric.calculate_metric_scores(answer_golds, answer_preds)

    avg_sf_p = np.mean(sf_metrics["precision"])
    avg_sf_r = np.mean(sf_metrics["recall"])
    avg_sf_f1 = np.mean(sf_metrics["f1"])

    print("="*70)
    print(" HOTPOTQA EVALUATION RESULTS ")
    print("="*70)
    print(f"Answer Exact Match : {em_res['ExactMatch']:.4f}")
    print(f"Answer F1 Score    : {f1_res['F1']:.4f}")
    print("-"*50)
    print(f"Supp. Facts Prec.  : {avg_sf_p:.4f}")
    print(f"Supp. Facts Recall : {avg_sf_r:.4f}")
    print(f"Supp. Facts F1     : {avg_sf_f1:.4f}")
    print(f"Joint F1 (Ans + SF): {f1_res['F1'] * avg_sf_f1:.4f}")
    print("="*70)


if __name__ == "__main__":
    main()